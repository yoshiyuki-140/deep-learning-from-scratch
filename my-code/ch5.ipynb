{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗算レイヤの実装\n",
    "\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加算レイヤの実装\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 図5-17の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "## リンゴ一つの値段と個数を掛け算\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "## オレンジ一つの値段と個数を掛け算\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "## オレンジとリンゴの値段の和\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "## 消費税率をかける\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLUレイヤの計算グラフ\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None  # True/FalseからなるNumpy配列\n",
    "        # xも同様にTrue/Falseからなる配列\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        \"\"\"順伝播(じゅんでんぱ)\"\"\"\n",
    "        self.mask = (\n",
    "            x <= 0\n",
    "        )  # xの値の中で、0以下の値の場所(インデックスとはちょっと違う)をTrueにし、それ以外はFalseにする。\n",
    "        out = (\n",
    "            x.copy()\n",
    "        )  # copyメソッドを使用してから複製を行うことで、参照先のメモリアドレスが違う場所になる。\n",
    "        out[self.mask] = 0  # 0以下の部分を0にする\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播(ぎゃくでんぱ)\"\"\"\n",
    "        dout[self.mask] = 0  # 0以下の部分を0にする。\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- シグモイド関数 $ y = \\frac{1}{1 + e^{-x}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out  # インスタンス変数へ保存\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine/Softmaxレイヤの実装\n",
    "今までやってきたニューラルネットワークの順電波の行列積は、幾何学分野では「アフィン変換」と呼ばれる。\n",
    "\n",
    "$ Y = X ・ W + B$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        # 重みバイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        self.x = x  # インスタンス変数へ保存\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # この処理で何をしているのかがわからない\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数と損失関数を結合したレイヤの構成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import cross_entropy_error, softmax\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 損失\n",
    "        self.y = None  # ソフトマックス関数の出力\n",
    "        self.t = None  # 教師データ(one-hot形式)\n",
    "\n",
    "    def forward(self, x: np.ndarray, t: np.ndarray):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)  # 損失関数の計算\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差逆伝播法を実装した、ニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import (\n",
    "    numerical_gradient,\n",
    ")  # 数値微分によって損失関数の勾配を求める関数\n",
    "from collections import OrderedDict  # 順序付き辞書型クラス\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\"2層ニューラルネットワーク\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みとバイアスの初期化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)  # このコードの意味がわからない\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W1\"], self.params[\"b1\"])\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 順伝播\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"最後のレイヤは損失関数のレイヤ\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"あるループ間でのモデル精度を計算\"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "        # 以下のif文による処理の解説 :\n",
    "        # ndimは、np.ndarray型の変数tの次元数を参照している\n",
    "        # これが1出なかったとき、つまり多次元配列出なかった場合のみ,\n",
    "        # np.argmaxで配列の中で最も値が大きい数値のインデックスを参照するメソッドを呼び出している.\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"数値微分による勾配計算\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): _description_\n",
    "            y (numpy.dnarray): _description_\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(\n",
    "            x, t\n",
    "        )  # loss_Wという関数を形式的に定義,Wはダミー変数で使われることはない\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"誤差逆伝播による勾配計算\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): _description_\n",
    "            t (numpy.ndarray): _description_\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1  # 初期値\n",
    "        dout = self.lastLayer.backward(dout)  # 損失関数の逆伝播\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers = reversed(layers)  # レイヤ順序反転\n",
    "\n",
    "        # 逆伝播\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W1\"], grads[\"b1\"] = self.layers[\"Affine1\"].dW, self.layers[\"Affine1\"].db\n",
    "        grads[\"W2\"], grads[\"b2\"] = self.layers[\"Affine2\"].dW, self.layers[\"Affine2\"].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "        # なぜ、数値微分は計算に時間がかかるの?\n",
    "        # ここがそもそもわからない."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配確認\n",
    "\n",
    "**勾配確認**とは,誤差逆伝播法によって求められた勾配と、数値微分によって求められた勾配の差分を比較し、\n",
    "誤差逆伝播法が正しく実装されているかを確認する作業である。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.996743932387138e-10\n",
      "b1:2.663029389046072e-09\n",
      "W2:6.244039074375628e-09\n",
      "b2:1.402270424805119e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)  # Backpropagation:誤差逆伝播法の意\n",
    "\n",
    "# 各重みの絶対誤差の平均値を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(f\"{key}:{str(diff)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法による、学習!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09736666666666667 0.0982\n",
      "0.7824833333333333 0.7889\n",
      "0.8753833333333333 0.8783\n",
      "0.89815 0.9004\n",
      "0.9081 0.9099\n",
      "0.9143166666666667 0.9157\n",
      "0.9192666666666667 0.9213\n",
      "0.92305 0.9256\n",
      "0.9279 0.9298\n",
      "0.93055 0.9326\n",
      "0.9340166666666667 0.9345\n",
      "0.9362833333333334 0.9378\n",
      "0.93805 0.9384\n",
      "0.9407666666666666 0.9417\n",
      "0.9422166666666667 0.9413\n",
      "0.9444333333333333 0.9438\n",
      "0.946 0.9458\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  # ここは適宜変更\n",
    "train_size = x_train.shape[0]\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []  # 損失関数の値のリスト\n",
    "train_acc_list = []  # 訓練データによるモデル精度のリスト\n",
    "test_acc_list = []  # テストデータによるモデル精度のリスト\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(\n",
    "        train_size, batch_size\n",
    "    )  # 60000からbatch_size(多分100)個のデータを無作為抽出\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 誤差逆電波法によって勾配を求める\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # update hyper parameters\n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1epochごとの処理\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dezero_black",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
