{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutionレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T  # フィルタの展開 -> 1次元に成形\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 展開\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h, self.pool_w)\n",
    "\n",
    "        # 最大値\n",
    "        out = np.max(col, axis=1)\n",
    "        # 成型\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全体実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import Relu, Affine, SoftmaxWithLoss\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=(1, 28, 28),\n",
    "        conv_param={\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1},\n",
    "        hidden_size=100,\n",
    "        output_size=10,\n",
    "        weight_init_std=0.01,\n",
    "    ):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        filter_stribe = conv_param[\"stribe\"]\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (\n",
    "            input_size - filter_size + 2 * filter_pad\n",
    "        ) / filter_stribe + 1\n",
    "\n",
    "        pool_output_size = int(\n",
    "            filter_num * (conv_output_size / 2) * (conv_output_size / 2)\n",
    "        )\n",
    "\n",
    "        # 重みパラメータの初期化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(\n",
    "            filter_num, input_dim[0], filter_size, filter_size\n",
    "        )\n",
    "        self.params[\"b1\"] = np.zeros(filter_num)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(\n",
    "            pool_output_size, hidden_size\n",
    "        )\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W3\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Convolution(\n",
    "            self.params[\"W1\"],\n",
    "            self.params[\"b1\"],\n",
    "            self.params[\"stride\"],\n",
    "            self.params[\"pad\"],\n",
    "        )\n",
    "\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配\"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Conv1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Conv1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W3\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b3\"] = self.layers[\"Affine2\"].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNによる学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m\n\u001b[1;32m     20\u001b[0m network \u001b[38;5;241m=\u001b[39m SimpleConvNet(\n\u001b[1;32m     21\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m),\n\u001b[1;32m     22\u001b[0m     conv_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     29\u001b[0m     network,\n\u001b[1;32m     30\u001b[0m     x_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# パラメータの保存\u001b[39;00m\n\u001b[1;32m     43\u001b[0m network\u001b[38;5;241m.\u001b[39msave_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/lab/datascience/deep-learning-from-scratch/my-code/../common/trainer.py:99\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m---> 99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[1;32m    101\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Documents/lab/datascience/deep-learning-from-scratch/my-code/../common/trainer.py:62\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[1;32m     60\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[0;32m---> 62\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "File \u001b[0;32m~/Documents/lab/datascience/deep-learning-from-scratch/my-code/simple_convnet.py:147\u001b[0m, in \u001b[0;36mSimpleConvNet.gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"勾配を求める（誤差逆伝搬法）\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    grads['b1']、grads['b2']、...は各層のバイアス\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[1;32m    150\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/lab/datascience/deep-learning-from-scratch/my-code/simple_convnet.py:87\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"損失関数を求める\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    引数のxは入力データ、tは教師ラベル\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "File \u001b[0;32m~/Documents/lab/datascience/deep-learning-from-scratch/my-code/simple_convnet.py:79\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 79\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/lab/datascience/deep-learning-from-scratch/my-code/../common/layers.py:224\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    223\u001b[0m     FN, C, FH, FW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 224\u001b[0m     N, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    225\u001b[0m     out_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((H \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m-\u001b[39m FH) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[1;32m    226\u001b[0m     out_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m-\u001b[39m FW) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削除\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(\n",
    "    input_dim=(1, 28, 28),\n",
    "    conv_param={\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1},\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    weight_init_std=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    network,\n",
    "    x_train,\n",
    "    t_train,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    epochs=max_epochs,\n",
    "    mini_batch_size=100,\n",
    "    optimizer=\"Adam\",\n",
    "    optimizer_param={\"lr\": 0.001},  # 小さくね?\n",
    "    evaluate_sample_num_per_epoch=1000,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {\"train\": \"o\", \"test\": \"s\"}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, markers=\"o\", label=\"train\", markevery=2)\n",
    "plt.plot(x, trainer.test, markers=\"s\", label=\"test\", markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dezero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
